---
layout: post
title: "[TIL] 2025-06-05 / XGBoost"
date: 2025-06-05 20:40:00 +0900
categories: 
  - til
  - main-camp
---

* toc
{:toc}

## 📖 Today I Learned
### XGBoost

---

#### - **XGBoost**
- eXtreme Gradient Boosting의 줄임말  
- **결정 트리 기반의 앙상블 학습 알고리즘**  
- Gradient Boosting 방식에 다양한 최적화 기법을 적용하여 빠르고 정확하게 동작함  
- Kaggle, Dacon 등 머신러닝 대회에서 꾸준히 상위권 모델로 사용됨

#### - **Gradient Boosting 방식 사용**
- 이전 트리의 오차를 다음 트리가 보정하는 방식으로 학습 진행  
- 여러 개의 약한 학습기(결정 트리)를 순차적으로 학습하여 강한 예측기 생성

#### - **정규화 적용**
- LightGBM보다 먼저 L1, L2 정규화 기법을 도입함  
- 과적합 방지에 효과적임

#### - **병렬 처리 최적화**
- 노드 분할 시 가능한 조건을 병렬로 계산함  
- 멀티코어 CPU 사용 시 학습 속도 빠름

#### - **결측값 자동 처리**
- 학습 중 결측값을 따로 처리하지 않아도 됨  
- 자동으로 최적 분기 방향 결정함

#### - 장점
- 높은 예측 정확도  
- 과적합 방지를 위한 정규화 기능 내장  
- 빠른 학습 속도  
- 결측값 처리 및 다양한 커스터마이징 가능  
- 다양한 문제 유형 지원 (회귀, 분류, 순위)

#### - 단점
- 트리 기반 모델 특성상 해석 어려움 (블랙박스 성향)  
- 하이퍼파라미터 많아서 튜닝 복잡함  
- 텍스트, 이미지 등 비정형 데이터 처리에는 부적합함  
- 큰 데이터셋에서 메모리 사용량 높을 수 있음

#### - 주요 하이퍼파라미터

| 파라미터         | 설명                                |
|------------------|-------------------------------------|
| `n_estimators`   | 트리 개수                            |
| `max_depth`      | 트리 깊이 제한                        |
| `learning_rate`  | 학습률                                |
| `subsample`      | 학습에 사용할 샘플 비율                |
| `colsample_bytree` | 트리 생성 시 사용할 피처 비율         |
| `gamma`          | 노드 분할의 최소 손실 감소 기준         |
| `reg_alpha` / `reg_lambda` | L1, L2 정규화 계수       |

#### - 파이썬 사용 예시

```python
import xgboost as xgb
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y)

model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
model.fit(X_train, y_train)
pred = model.predict(X_test)
```

#### - 활용 분야
- 회귀 문제: 가격, 수요, 지수 예측  
- 이진 분류: 이탈 예측, 사기 탐지  
- 다중 분류: 고객 분류, 품질 예측  
- 순위 문제: 추천 시스템, 검색 결과 정렬

#### - LightGBM과 비교 요약

| 항목             | XGBoost                     | LightGBM                    |
|------------------|-----------------------------|-----------------------------|
| 트리 성장 방식    | Level-wise (수평적)          | Leaf-wise (손실 기준 리프 확장) |
| 처리 속도        | 느린 편                      | 빠름                         |
| 과적합 방지      | L1/L2 정규화 효과적          | 빠른 학습에 따른 과적합 위험 있음 |
| 학습 데이터 형태 | Dense, Sparse 모두 지원     | Sparse에 특히 강함            |
| 커뮤니티         | 더 오래되고 안정적임          | 상대적으로 최신 프레임워크     |

---
